{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertTokenizer, BertForQuestionAnswering\nimport torch\n\n#import warnings\n#warnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:55:04.067115Z","iopub.execute_input":"2025-12-26T08:55:04.067485Z","iopub.status.idle":"2025-12-26T08:55:41.601555Z","shell.execute_reply.started":"2025-12-26T08:55:04.067453Z","shell.execute_reply":"2025-12-26T08:55:41.600100Z"}},"outputs":[{"name":"stderr","text":"2025-12-26 08:55:22.881392: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766739323.136932      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766739323.210961      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766739323.847104      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766739323.847184      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766739323.847190      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766739323.847194      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# squad veri seti üzerinde ince ayar yapılmış bert modeli\nmodel_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n\n# bert tokenizer\ntokenizer = BertTokenizer.from_pretrained(model_name) # önceden eğitilmiş BERT modeline ait tokenizer'ı yükleme\n\n# soru cevaplama görevi için ince ayar yapılmış bert modeli\nmodel = BertForQuestionAnswering.from_pretrained(model_name) #önceden eğitilmiş BERT tabanlı soru-cevap modelini yükleme\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:55:41.604642Z","iopub.execute_input":"2025-12-26T08:55:41.605394Z","iopub.status.idle":"2025-12-26T08:55:50.576240Z","shell.execute_reply.started":"2025-12-26T08:55:41.605357Z","shell.execute_reply":"2025-12-26T08:55:50.575059Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28e37e81e61d4768a001898b2aaf29a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742e0f7fd1da4efe9964e07c8b346e4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead38861da5a48b2b7478fbaf13728db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71144eb71dab4b94bfb50af303a52d50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50373e49d1ab41099cc835ce39bcb262"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# cevapları tahmin eden fonksiyon\ndef predict_answer(context, question):\n\n    # metni ve soruyu tokenlara ayirma ve modele uygun hale getirme\n    encoding = tokenizer.encode_plus(question, context, return_tensors=\"pt\", max_length=512, truncation=True)\n    # Soru ve bağlamı (context) BERT'in anlayacağı şekilde tokenize eder,\n    # PyTorch tensörleri olarak döndürür, maksimum uzunluğu 512 token ile sınırlar\n    # ve gerekirse metni kırpar (truncation=True)\n\n    # giriş tensorlerini hazırlama\n    input_ids = encoding[\"input_ids\"] # tokenlerin idsi\n    attention_mask = encoding[\"attention_mask\"] # hangi tokenlarin dikkate alinacagini belirtir\n\n    # modeli calistirma ve skorları hesaplama\n    with torch.no_grad(): # Gradientsiz (eğitim yapmadan) model çalıştırmak için no_grad bloğu kullanılır\n        start_scores, end_scores = model(input_ids, attention_mask=attention_mask, return_dict = False)\n        # Modeli çalıştırarak cevabın başlangıç ve bitiş pozisyonları için skorları alır\n        # return_dict=False: çıktının tuple (demet) olarak dönmesini sağlar\n\n    # en yüksek olasılığa sahip start ve end indekslerini hesaplama [1.2, 2.4, 0.3, 2.5,4.7]\n    start_index = torch.argmax(start_scores, dim=1).item() # baslangic indeks\n    end_index = torch.argmax(end_scores, dim=1).item() # bitis indeksimiz\n\n    # token id lerini kullanarak cevap metnini elde etme\n    answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[0][start_index: end_index + 1])\n    # Modelin belirlediği başlangıç ve bitiş indeksleri arasındaki token ID'lerini\n    # gerçek BERT token'larına (kelime parçacıklarına) dönüştürür\n\n\n    # tokenları birleştirme ve okunabilir hale getirme\n    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:55:50.577458Z","iopub.execute_input":"2025-12-26T08:55:50.577758Z","iopub.status.idle":"2025-12-26T08:55:50.586161Z","shell.execute_reply.started":"2025-12-26T08:55:50.577699Z","shell.execute_reply":"2025-12-26T08:55:50.584951Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"question = \"What is the capital of Turkey?\"\ncontext = \"The capital city of Turkey is Ankara.\"\n\nanswer = predict_answer(context, question)\nprint(\"Answer:\", answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:55:50.587261Z","iopub.execute_input":"2025-12-26T08:55:50.588283Z","iopub.status.idle":"2025-12-26T08:55:50.982287Z","shell.execute_reply.started":"2025-12-26T08:55:50.588249Z","shell.execute_reply":"2025-12-26T08:55:50.981301Z"}},"outputs":[{"name":"stdout","text":"Answer: ankara\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"question = '''What is Natural Language Processing?'''\ncontext = ''' Natural Language Processing (NLP) is a field of artificial intelligence that enables computers to understand and process human language.\n              It analyzes both written and spoken language to extract meaning, answer questions, and generate appropriate responses.\n              Technologies such as translation apps, chatbots, voice assistants, and sentiment analysis systems all rely on NLP.\n              Because human language is complex and often ambiguous, interpreting it accurately presents significant challenges.\n              Therefore, NLP continues to evolve through the use of machine learning and deep learning techniques. '''\n\n\nanswer = predict_answer(context, question)\nprint(\"Answer:\", answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:55:50.983698Z","iopub.execute_input":"2025-12-26T08:55:50.984131Z","iopub.status.idle":"2025-12-26T08:55:51.755977Z","shell.execute_reply.started":"2025-12-26T08:55:50.984091Z","shell.execute_reply":"2025-12-26T08:55:51.755079Z"}},"outputs":[{"name":"stdout","text":"Answer: a field of artificial intelligence\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#ingilizce için\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:55:51.757073Z","iopub.execute_input":"2025-12-26T08:55:51.757363Z","iopub.status.idle":"2025-12-26T08:55:56.630480Z","shell.execute_reply.started":"2025-12-26T08:55:51.757335Z","shell.execute_reply":"2025-12-26T08:55:56.629148Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb712fe367246df88e112e6bfa159aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7deb0cbac6764caa9e92b74e1c3d586c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1093019db79c46dd9e3bd61b4e65609b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b4e8428d324b9ebbe135b5fe84245a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edfe9dc19d49450b909df9f9e62bfb56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"540528c36e19444683815bee8d191eed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e00bbd21e754c5882c8aacec52e3b1c"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"#Türkçe için\nmodel_name = \"ytu-ce-cosmos/turkish-gpt2-large\"   # Türkçe GPT-2 modeli\n\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:55:56.633500Z","iopub.execute_input":"2025-12-26T08:55:56.634023Z","iopub.status.idle":"2025-12-26T08:56:15.451144Z","shell.execute_reply.started":"2025-12-26T08:55:56.633983Z","shell.execute_reply":"2025-12-26T08:56:15.449921Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f576c6f4075b42d98bdd07be43605b20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b676a5ebd1394866b39ee354e0608bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"287be5dbf0c54e48acb09faafe6d86c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848a89e295304c23b0c15c987258fbf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bad848a81704c01bdd873ee8c83aaf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/894 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d0cda49c3ac4c7d8c3f895251360b81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.10G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a86da3317a41028a79d4f9333d796a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c6e96ddf0048bf98a12ef1a28a9068"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"#metin üretmek için fonksiyon\ndef generate_text(prompt, max_length=150):\n    # prompt'u token'lara çeviriyoruz ve PyTorch tensörü olarak döndürüyoruz.\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n\n    outputs = model.generate(\n        inputs,                 # modele başlangıç token'larını veriyoruz\n        max_length=max_length,  # üretilecek maksimum uzunluk\n        temperature=0.8,        # yaratıcılık düzeyi (yüksek → daha yaratıcı)\n        top_p=0.9,              # nucleus sampling: %90 olasılık kütlesi içinden seçim\n        do_sample=True,         # rastgele örneklemeyi aktif eder\n        pad_token_id=tokenizer.eos_token_id  # eksik token varsa onları EOS ile doldur\n    )\n    # üretilen token'ları tekrar metne çeviriyoruz\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:56:15.452331Z","iopub.execute_input":"2025-12-26T08:56:15.452641Z","iopub.status.idle":"2025-12-26T08:56:15.460270Z","shell.execute_reply.started":"2025-12-26T08:56:15.452611Z","shell.execute_reply":"2025-12-26T08:56:15.458854Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"text = generate_text(\"Today I am learning NLP \")\nprint(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:56:15.461687Z","iopub.execute_input":"2025-12-26T08:56:15.462046Z","iopub.status.idle":"2025-12-26T08:56:48.833631Z","shell.execute_reply.started":"2025-12-26T08:56:15.462016Z","shell.execute_reply":"2025-12-26T08:56:48.832498Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Today I am learning NLP \"Consciousness\" (Zeka ile Bağlantılı)\nKonu: Today I am learning NLP \"Consciousness\" (Zeka ile Bağlantılı)\n07.Aralık.2012, 21:10 #1\nToday I am learning NLP \"Consciousness\" (Zeka ile Bağlantılı)\nZeka ve \"Consciousness\" kavramlarının, günlük yaşamımızda ne gibi etkileri olabilir?\nZeka ve \"Consciousness\" kavramlarının günlük yaşamımızda ne gibi etkileri olabilir? Bu kavramların günlük yaşamımızdaki etkileri nelerdir?\nZeka, yaşamın her döneminde bir çok alanda karşılaştığımız bir\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"#!pip install transformers torch gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:56:48.834769Z","iopub.execute_input":"2025-12-26T08:56:48.835085Z","iopub.status.idle":"2025-12-26T08:56:48.840361Z","shell.execute_reply.started":"2025-12-26T08:56:48.835057Z","shell.execute_reply":"2025-12-26T08:56:48.839660Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import gradio as gr\n\ndef gradio_generate(prompt):\n    return generate_text(prompt)\n\ndemo = gr.Interface(\n    fn=gradio_generate,\n    inputs=\"text\",\n    outputs=\"text\",\n    title=\"GPT-2 ile Metin Üretimi Uygulaması\",\n    description=\"Bir başlangıç cümlesi girin, GPT-2 devamını sizin için yazsın!\"\n)\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T08:56:48.841402Z","iopub.execute_input":"2025-12-26T08:56:48.841696Z","iopub.status.idle":"2025-12-26T08:56:58.970434Z","shell.execute_reply.started":"2025-12-26T08:56:48.841668Z","shell.execute_reply":"2025-12-26T08:56:58.969345Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://eae3d6f4d82714437b.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://eae3d6f4d82714437b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":11}]}